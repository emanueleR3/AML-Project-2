\section{Experiments}
\label{sec:experiments}

In this section, we present our experimental setup and analyze the results obtained across different federated learning configurations. We begin by establishing strong centralized and federated baselines, followed by an in-depth analysis of our proposed sparse fine-tuning method, including ablation studies and a comparison of different gradient masking strategies.

\subsection{Dataset and Metrics}
\label{subsec:dataset}

We evaluate our methods on \textbf{CIFAR-100}, a widely used benchmark in image classification containing 60,000 32$\times$32 color images across 100 fine-grained classes. The dataset is split into 50,000 training images and 10,000 test images. We further partition the training set to create a 90/10 train/validation split for hyperparameter tuning.

We report the following metrics:
\begin{itemize}
    \item \textbf{Test Accuracy (\%):} Classification accuracy on the held-out test set.
    \item \textbf{Cross-Entropy Loss:} Standard loss function for multi-class classification.
\end{itemize}

All experiments use a DINO ViT-S/16 backbone with a linear classification head, trained with the SGDM optimizer.

\subsection{Baselines}
\label{subsec:baselines}

We establish two key baselines to contextualize our federated learning results:

\paragraph{Centralized Training.}
This represents the empirical upper bound, where all data is available on a single machine. We trained the model using the SGDM optimizer with a learning rate of $10^{-5}$ and batch size of 64 for 16 epochs. Figure~\ref{fig:central_curves} shows the training dynamics.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/central_baseline_curves.pdf}
    \caption{Centralized baseline training metrics with Cosine Annealing scheduler.}
    \label{fig:central_curves}
\end{figure}

\paragraph{Learning Rate Scheduler Comparison.}
We compared four learning rate schedulers: Cosine Annealing, Step LR, Exponential, and Reduce on Plateau. Figure~\ref{fig:scheduler} shows the test accuracy progression for each scheduler.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/scheduler_comparison.pdf}
    \caption{Comparison of learning rate schedulers. Cosine Annealing achieves the best final test accuracy (86.18\%), followed by Exponential (85.96\%), Plateau (85.87\%), and Step (85.68\%).}
    \label{fig:scheduler}
\end{figure}

The results show that \textbf{Cosine Annealing} achieves the best performance with 86.18\% test accuracy, outperforming Step LR by 0.5 percentage points. The smooth learning rate decay provided by Cosine Annealing allows for more stable convergence, while Step LR's abrupt changes can lead to suboptimal final performance.

\paragraph{FedAvg Standard (IID).}
The standard Federated Averaging algorithm serves as our FL baseline. With $K=100$ clients, $C=0.1$ participation rate, and $J=4$ local steps per round, FedAvg achieves \textbf{36.93\%} test accuracy after 100 rounds. Extending training to 300 rounds improves this to \textbf{57.99\%}. The significant gap from the centralized baseline (approx. 11\%) illustrates the inherent challenges of distributed optimization, even under favorable IID conditions. Figure~\ref{fig:fedavg_iid} shows the convergence behavior.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/fedavg_iid_convergence.pdf}
    \caption{FedAvg IID convergence over 300 rounds. While stable, it requires significantly more rounds to approach centralized performance.}
    \label{fig:fedavg_iid}
\end{figure}

\subsection{Impact of Data Heterogeneity}
\label{subsec:noniid}

To study the effect of statistical heterogeneity, we simulated non-IID partitions by restricting each client to $N_c$ classes. We conducted a systematic sweep over $N_c \in \{1, 5, 10, 50\}$ and local steps $J \in \{4, 8, 16\}$, scaling the number of rounds to keep the total computation constant budget.

Table~\ref{tab:noniid} (and Figure~\ref{fig:noniid_heatmap}) summarizes the results. We observe that:
\begin{enumerate}
    \item \textbf{Heterogeneity degrades performance:} Accuracy drops from 66.05\% ($N_c=50$, near-IID) to 25.15\% ($N_c=1$, extreme non-IID).
    \item \textbf{Local steps help in IID settings:} Increasing local computation ($J=16$) is beneficial when data is homogeneous ($N_c=50$), boosting accuracy by over 28pp.
    \item \textbf{Local steps hurt in non-IID settings:} Under extreme heterogeneity ($N_c=1$), increasing local steps yields diminishing returns (only +5.4pp), as local models drift too far from the global objective.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/noniid_scaled_heatmap.pdf}
    \caption{Test accuracy across varying heterogeneity ($N_c$) and local computation ($J$). High heterogeneity ($N_c=1$) severely limits performance.}
    \label{fig:noniid_heatmap}
\end{figure}

\subsection{Sparse Fine-Tuning: Ablation Studies}
\label{subsec:ablation}

Before deploying our Sparse FedAvg, we conducted ablation studies to determine optimal hyperparameters for the Fisher Information masking.

\paragraph{Calibration Rounds.}
We tested calibrating the Fisher Information matrix over 1, 3, 5, and 10 rounds. Results showed that performance saturates quickly; increasing calibration rounds beyond 3 provided negligible gains. We selected \textbf{3 rounds} as a trade-off between calibration cost and mask quality.

\paragraph{Sparsity Ratio.}
We experimented with keeping 10\%, 20\%, and 40\% of parameters (sparsity 90\%, 80\%, 60%). We found that an \textbf{80\% sparsity} (updating only 20\% of weights) preserved enough capacity for adaptation while sufficiently reducing interference, achieving the best balance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/ablation_studies.pdf}
    \caption{Ablation studies for calibration rounds and sparsity ratio. 3 calibration rounds and 80\% sparsity were selected as optimal.}
    \label{fig:ablation}
\end{figure}

\subsection{Extension: Comparison of Masking Rules}
\label{subsec:extension}

In this section, we address the core of our proposed extension: evaluating different criteria for selecting which parameters to update. We compared five strategies using 80\% sparsity on the non-IID setting ($N_c=5$):

\begin{enumerate}
    \item \textbf{Least-Sensitive (Ours):} Updates parameters with the lowest Fisher Information. Rationale: adapt weights that matter least to pre-trained knowledge.
    \item \textbf{Most-Sensitive:} Updates parameters with highest Fisher Information.
    \item \textbf{Lowest-Magnitude:} Updates weights close to zero.
    \item \textbf{Highest-Magnitude:} Updates weights with large values.
    \item \textbf{Random:} Selects a random subset of parameters.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{output/figures/masking_rule_comparison.pdf}
    \caption{Comparison of different masking strategies in a non-IID setting ($N_c=5$). The Fisher-based methods (Sensitivity) outperform magnitude-based and random selection.}
    \label{fig:masking_rules}
\end{figure}

\paragraph{Results.} 
Figure~\ref{fig:masking_rules} illustrates the comparative performance.
Our experiments reveal that \textbf{Least-Sensitive} masking achieves the highest accuracy (\textbf{76.32\%} at $J=4$), confirming our hypothesis that preserving high-information weights mitigates catastrophic forgetting. 
Interestingly, \textbf{Most-Sensitive} masking also performed competitively, suggesting that adaptation in high-information regions can be effective if carefully controlled, though it risks overriding pre-trained features.
Both magnitude-based methods and random masking performed significantly worse. Specifically, Random masking reached only ~26.14\% in extreme non-IID settings ($N_c=1$), highlighting that intelligent parameter selection is crucial for sparse federated learning.

These findings validate that Fisher Information provides a meaningful signal for parameter importance in the context of Federated Learning, enabling targeted updates that reduce interference while maintaining plasticity.
