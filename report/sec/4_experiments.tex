\section{Experiments}
\label{sec:experiments}

In this section, we present the experimental evaluation of our proposed methods. We first detail the experimental setup, including the dataset and model architecture. We then establish strong centralized and federated baselines to benchmark performance. Finally, we analyze the impact of our sparse fine-tuning strategy through ablation studies and a comparative analysis of different gradient masking rules.

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Dataset and Model.}
We conduct all experiments on the \textbf{CIFAR-100} dataset, which consists of 60,000 32$\times$32 color images in 100 classes, with 500 training images and 100 test images per class. We use a pre-trained \textbf{DINO ViT-S/16} backbone with a linear classification head. We adopted a two-stage training strategy: first, we froze the backbone and trained only the linear head; subsequently, we froze the trained head and fine-tuned the entire backbone. All models are trained using the SGD optimizer with momentum (SGDM).

\paragraph{Federated Setting.}
We simulate a federated environment with $K=100$ clients and a participation rate of $C=0.1$ (10 clients per round). We explore both IID and non-IID data partitions. For non-IID settings, we restrict each client to hold data from only $N_c$ classes, creating statistical heterogeneity.

\subsection{Centralized Baseline}
\label{subsec:centralized}

To establish an upper bound on performance, we evaluated the model in a centralized setting. We compared four learning rate schedulers: Cosine Annealing, ReduceLROnPlateau, Exponential, and StepLR. The results are summarized in Table~\ref{tab:centralized_schedulers}.

\begin{table}[h]
\centering
\caption{Centralized Baseline Scheduler Comparison}
\label{tab:centralized_schedulers}
\begin{tabular}{lr}
\toprule
Scheduler & Test Accuracy (\%) \\
\midrule
Cosine & 86.18 \\
Plateau & 85.87 \\
Exponential & 85.96 \\
Step & 85.68 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{output/figures/central_baseline_curves.png}
    \caption{Centralized baseline training curves comparing different learning rate schedulers.}
    \label{fig:central_curves}
\end{figure*}

As shown in Table~\ref{tab:centralized_schedulers}, the \textbf{Cosine Annealing} scheduler achieved the highest final test accuracy. We observed that the model converges rapidly. Consequently, we adopt Cosine Annealing for subsequent experiments.



\subsection{Federated Learning Baselines}
\label{subsec:fl_baselines}

We next evaluate the standard FedAvg algorithm under various conditions to understand the impact of data heterogeneity and local computation.

We varied the number of classes per client $N_c \in \{1, 5, 10, 50\}$ to simulate varying degrees of non-IID data and investigated the effect of the number of local update steps $J \in \{4, 8, 16\}$. Figure~\ref{fig:heterogeneity_heatmap} provides a comprehensive summary of these results.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{output/figures/noniid_scaled_heatmap.png}
    \caption{Impact of statistical heterogeneity ($N_c$) and local steps ($J$) on test accuracy.}
    \label{fig:heterogeneity_heatmap}
\end{figure}

We observe a significant performance degradation as heterogeneity increases ($N_c$ decreases), while increasing local steps generally benefits performance in IID or near-IID settings but can be detrimental in highly heterogeneous environments due to client drift.

\subsection{Sparse Fine-Tuning Optimizations}
\label{subsec:sparse}

\subsubsection{Ablation Studies}
We conducted ablation studies to determine the optimal hyperparameters for our sparse fine-tuning method, specifically the number of Fisher calibration rounds and the sparsity ratio.

\begin{table}[h]
\centering
\caption{Sparse Ablation: Sparsity Levels and Calibration Rounds}
\label{tab:sparse_ablation}
\begin{tabular}{llr}
\toprule
Type & Value & Test Accuracy (\%) \\
\midrule
Calibration Rounds & 1 & 77.87 \\
Calibration Rounds & 3 & 77.91 \\
Calibration Rounds & 5 & 77.91 \\
Calibration Rounds & 10 & 77.90 \\
Sparsity & 20\% & 80.10 \\
Sparsity & 50\% & 77.92 \\
Sparsity & 90\% & 73.81 \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:sparse_ablation} and Figure~\ref{fig:ablation}, calculating Fisher Information over \textbf{3 rounds} is sufficient for stable masking. Regarding sparsity, we found that maintaining \textbf{20\% sparsity} provided the optimal balance between plasticity and stability.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{output/figures/ablation_studies.png}
    \caption{Ablation study results for calibration rounds and sparsity levels.}
    \label{fig:ablation}
\end{figure}

\subsection{Extension: Comparison of Masking Rules}
\label{subsec:extension_results}

Finally, we evaluated five different strategies for selecting which parameters to update (Masking Rules) in a non-IID setting. We compared:
\begin{itemize}
    \item \textbf{Least Sensitive:} Updates parameters with the smallest Fisher Information.
    \item \textbf{Most Sensitive:} Updates parameters with the largest Fisher Information.
    \item \textbf{Lowest Magnitude:} Updates parameters with the smallest absolute values.
    \item \textbf{Highest Magnitude:} Updates parameters with the largest absolute values.
    \item \textbf{Random:} Selects parameters randomly.
\end{itemize}

Table~\ref{tab:masking_rules} and Figure~\ref{fig:masking_rules} present the comparative results.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{output/figures/masking_rule_comparison.png}
    \caption{Performance comparison of different gradient masking rules across various non-IID settings.}
    \label{fig:masking_rules}
\end{figure*}
\begin{table}[h]
\centering
\caption{Comparison of Masking Rules in Non-IID Settings}
\label{tab:masking_rules}
\begin{tabular}{rrlr}
\toprule
$N_c$ & $J$ & Masking Rule & Test Accuracy (\%) \\
\midrule
5 & 4 & Highest Magnitude & 81.16 \\
5 & 4 & Least Sensitive & 79.32 \\
5 & 4 & Lowest Magnitude & 80.53 \\
5 & 4 & Most Sensitive & 80.18 \\
5 & 4 & Random & 80.61 \\
\addlinespace
5 & 8 & Highest Magnitude & 80.14 \\
5 & 8 & Least Sensitive & 79.62 \\
5 & 8 & Lowest Magnitude & 77.17 \\
5 & 8 & Most Sensitive & 78.40 \\
5 & 8 & Random & 78.95 \\
\addlinespace
10 & 8 & Highest Magnitude & 82.09 \\
10 & 8 & Least Sensitive & 80.60 \\
10 & 8 & Lowest Magnitude & 80.00 \\
10 & 8 & Most Sensitive & 79.93 \\
10 & 8 & Random & 81.17 \\
\bottomrule
\end{tabular}
\end{table}


The \textbf{Highest Magnitude} pruning rule consistently outperformed other methods, including Fisher-based sensitivity methods (Least/Most Sensitive). This suggests that in this transfer learning setup, adapting strong feature detectors (large magnitude weights) is particularly effective.
