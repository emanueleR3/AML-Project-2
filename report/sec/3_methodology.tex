\section{Methodology}
\label{sec:method}

This section details the technical implementation of our experiments. We describe the baseline setups, data partitioning strategies, the sparse fine-tuning pipeline, and our extension comparing different parameter selection rules.

\subsection{Experimental Setup}

\subsubsection{Centralized Baseline}

As an upper bound for federated performance, we implement centralized training where all data is available on a single machine. We use a pretrained DINO ViT-S/16 backbone and optimize a CIFAR-100 classifier using cross-entropy loss.

To mirror the federated protocol, we adopt a two-stage training procedure:
\begin{enumerate}
    \item \textbf{Head pre-training.} We freeze the DINO backbone and train only a linear classification head for 20 epochs using SGDM with Cosine Annealing. The best validation checkpoint is saved.
    \item \textbf{Backbone fine-tuning.} Starting from the pretrained head, we freeze the classifier and fine-tune the entire backbone for 40 epochs using SGDM with a lower learning rate and Cosine Annealing.
\end{enumerate}

\subsubsection{Federated Learning Simulation}

To simulate distributed training on a single GPU, we implement sequential client training. In each communication round:
\begin{enumerate}
    \item The server broadcasts the global model weights to selected clients.
    \item Each client trains independently on its local data partition for $J$ local steps (minibatch gradient updates).
    \item The server collects and aggregates client updates via weighted averaging based on each client's sample count.
\end{enumerate}

We adopt Federated Averaging (FedAvg) as our aggregation strategy. We fix $K=100$ total clients and a participation fraction of $C=0.1$, meaning 10 clients are sampled per round.

\subsection{Dataset Partitioning}

\subsubsection{Train/Validation Split}

We use CIFAR-100, which contains 50,000 training images across 100 classes. Images are resized to 224$\times$224 pixels and normalized with ImageNet statistics; during training we apply standard augmentations (random crop, horizontal flip, and RandAugment). Before client partitioning, we reserve 10\% of the training data (5,000 images) as a global validation set for model selection. The remaining 45,000 images are distributed among clients.

\subsubsection{IID Sharding}

For IID (independent and identically distributed) partitioning, the training set is shuffled and uniformly divided among all clients. Each client receives approximately equal representation of all 100 classes, mimicking the global distribution.

\subsubsection{Non-IID Sharding}

To simulate label distribution skew, we restrict each client to samples from only $N_c$ distinct classes. The parameter $N_c$ controls heterogeneity severity:
\begin{itemize}
    \item $N_c = 1$: Each client sees only one class (extreme heterogeneity).
    \item $N_c = 5, 10, 50$: Intermediate levels of heterogeneity.
    \item $N_c = 100$ (IID): Each client sees all classes.
\end{itemize}
This procedure yields label-skewed client datasets while keeping the overall sample counts approximately balanced.

\subsubsection{Scaled Rounds}

When increasing local steps $J$, we proportionally reduce communication rounds to maintain constant total computation. We fix a reference budget at $J=4$ local steps and 200 rounds (800 total steps), and scale accordingly for $J \in \{4, 8, 16\}$. This enables fair comparison across different values of $J$.

\subsection{Sparse Fine-Tuning Pipeline}

To mitigate interference between heterogeneous clients, we restrict model updates to a subset of parameters during federated training. Our pipeline consists of mask calibration followed by masked local training.

\subsubsection{Parameter Sensitivity via Fisher Information}

We identify parameter importance using the diagonal of the empirical Fisher Information Matrix. This metric estimates how much each parameter affects the model's predictions. High Fisher values indicate parameters critical to the pretrained knowledge, while low values indicate parameters that can be modified with minimal impact on existing capabilities.

\subsubsection{Multi-Round Mask Calibration}

We compute Fisher scores over multiple calibration rounds rather than a single pass. Each calibration round samples a subset of clients and estimates Fisher scores on each; we then average estimates across clients and accumulate across rounds. This multi-round approach yields more stable sensitivity estimates. After calibration, we apply global thresholding to create a binary mask at the desired sparsity ratio.

\subsubsection{SparseSGDM Optimizer}

We implement SparseSGDM, extending standard SGD with momentum to incorporate gradient masking. The optimizer applies the binary mask to both gradients and the weight decay term, ensuring that masked parameters receive no updates and remain frozen throughout training. In the federated setting, all clients share the same global mask, ensuring they update identical parameter subsets. This coordination maintains compatibility during aggregation even when local data distributions differ substantially.

\subsection{Extension: Alternative Masking Rules}

The core of our extension is a systematic comparison of different criteria for selecting which parameters to update. While the standard approach uses Fisher Information to select the least-sensitive parameters, we investigate whether alternative selection rules might be effective in the federated setting.

\subsubsection{Masking Strategies}

Given a sparsity ratio (fraction of parameters to freeze), we compare five selection rules:

\paragraph{Least-Sensitive (Baseline).} Select parameters with the lowest Fisher scores. Rationale: these parameters contribute least to the pretrained knowledge and can be safely modified without catastrophic forgetting.

\paragraph{Most-Sensitive.} Select parameters with the highest Fisher scores. Rationale: these parameters are most task-relevant and updating them may yield faster adaptation.

\paragraph{Lowest-Magnitude.} Select parameters with the smallest absolute weight values. Rationale: near-zero weights may be redundant and modifying them could add capacity without disrupting existing representations.

\paragraph{Highest-Magnitude.} Select parameters with the largest absolute weight values. Rationale: large weights dominate forward passes; updating them may have the strongest effect on model behavior.

\paragraph{Random.} Select parameters uniformly at random. This serves as a baseline to evaluate whether intelligent parameter selection provides meaningful benefits over naive approaches.

\subsubsection{Implementation Details}

For Fisher-based methods, scores are computed using the multi-round calibration procedure described above. For magnitude-based methods, we use the absolute values of the pretrained weights. For all methods, we apply global thresholding: we collect all scores, sort them, and select parameters according to the rule's criterion. This produces a binary mask that is distributed to all clients before federated training begins.
