\section{Methodology and Experimental Setup}
\label{sec:method}

In this section, we describe the implementation details of our federated simulation, the dataset partitioning strategies, and the integration of Task Arithmetic via sparse optimization.

\subsection{Dataset and Architecture}
We utilize the \textbf{CIFAR-100} dataset for all experiments. To leverage the power of self-supervised learning, we employ a pre-trained \textbf{DINO ViT-S/16} architecture. The backbone is fine-tuned on the downstream classification task. 
Since the standard CIFAR-100 provided by \texttt{torchvision} lacks a validation split, we partition the training set to create a local validation set for hyperparameter tuning.

\subsection{Federated Simulation and Heterogeneity}
To simulate a Federated Learning environment, we partition the training data among $K=100$ clients. We consider two distinct data distribution scenarios to evaluate robustness:
\begin{enumerate}
    \item \textbf{I.I.D. Sharding:} The training samples are shuffled and uniformly distributed among clients, ensuring each client holds a representative subset of the global distribution.
    \item \textbf{Non-I.I.D. Sharding:} To simulate statistical heterogeneity, each client is restricted to a subset of classes $N_c$. We control the severity of heterogeneity by varying $N_c \in \{1, 5, 10, 50\}$. A smaller $N_c$ indicates extreme non-IID conditions (e.g., $N_c=1$ implies a client sees only a single class).
\end{enumerate}

\subsection{Task Arithmetic and Sparse Optimization}
A core component of our investigation is the application of Task Arithmetic to reduce interference. We implement a custom optimizer, \textbf{SparseSGDM}, which modifies standard Stochastic Gradient Descent with Momentum by applying a binary mask $M$ to the gradients:
\begin{equation}
    w_{t+1} = w_t - \eta \cdot (M \odot \nabla \mathcal{L}(w_t))
\end{equation}
where $\odot$ denotes element-wise multiplication.

\paragraph{Gradient Mask Calibration.}
The mask $M$ is calibrated using the Fisher Information Matrix to identify parameter sensitivity. Following the approach in~\cite{iurada2025efficient}, we identify the "least-sensitive" parametersâ€”those that can be modified with minimal impact on the pre-trained knowledge.
The calibration process involves:
1. Computing the diagonal Fisher Information on a subset of data.
2. Selecting parameters with sensitivity scores below a specific threshold (determined by the desired sparsity ratio).
3. Fixing the mask $M$ for subsequent training rounds.

\subsection{Baselines and Training Protocol}
We compare the proposed sparse method against two primary baselines:
\begin{itemize}
    \item \textbf{Centralized Baseline:} Standard training on the full dataset using SGDM with Cosine Annealing scheduling.
    \item \textbf{Standard FedAvg:} Federated Averaging without gradient masking, serving as the benchmark for distributed performance.
\end{itemize}
For federated runs, we vary the number of local epochs $J \in \{4, 8, 16\}$ to analyze the trade-off between communication efficiency and model convergence (client drift).