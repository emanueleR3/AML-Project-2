\begin{abstract}
Federated Learning (FL) is a distributed learning paradigm that enables training on decentralized data without compromising privacy. However, a major challenge in FL is the statistical heterogeneity of data across clients, which often leads to interference during model aggregation and degrades global performance. In this work, we explore the intersection of Federated Learning and Task Arithmetic, leveraging Model Editing techniques to mitigate these issues. Specifically, we employ a pre-trained DINO ViT-S/16 architecture on the CIFAR-100 dataset and investigate the efficacy of Sparse Fine-Tuning. By restricting local updates to low-sensitivity parameters identified via Fisher Information, we aim to reduce interference and improve convergence in non-IID scenarios. Our approach is evaluated against centralized and standard federated baselines, analyzing the trade-offs between sparsity and model robustness.
\end{abstract}