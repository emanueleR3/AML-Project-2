\begin{abstract}
Federated Learning (FL) enables decentralized training of machine learning models while preserving data privacy. However, statistical heterogeneity among clients (non-IID data) often leads to model interference and performance degradation during aggregation. In this work, we analyze the intersection of Federated Learning and Model Editing, specifically focusing on Task Arithmetic and Sparse Fine-Tuning. Utilizing a pre-trained DINO ViT-S/16 architecture on the CIFAR-100 dataset, we investigate how restricting updates to low-sensitivity parameters affects convergence and global model performance. We compare standard Federated Averaging (FedAvg) against centralized baselines and explore the efficacy of masking gradients (SparseSGDM) to mitigate the adverse effects of data heterogeneity. Our experiments demonstrate the trade-offs between sparsity, communication rounds, and accuracy in varying degrees of non-IID scenarios.
\end{abstract}