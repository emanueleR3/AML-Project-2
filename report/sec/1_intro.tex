\section{Introduction}
\label{sec:intro}

The rapid growth of decentralized data generation has necessitated machine learning paradigms that can learn from distributed sources while respecting user privacy. Federated Learning (FL) addresses this by orchestrating a training process where a central server aggregates updates from local models trained on private client data. Despite its advantages, FL faces significant hurdles, primarily due to statistical heterogeneity (non-IID data) among clients. When local data distributions diverge, the resulting weight updates can conflict, a phenomenon known as "interference," which hinders the global model's ability to generalize.

Parallel to developments in FL, the field of Model Editing has introduced methods to modify or improve pre-trained models without full retraining. A promising direction is Task Arithmetic, which demonstrates that fine-tuned models can be merged by manipulating their task vectors. Recent research suggests that fine-tuning only a subset of "low-sensitivity" parameters—a technique known as Sparse Fine-Tuning—can facilitate the composition of knowledge from different sources with minimal interference.

In this paper, we aim to bridge these two fields by applying Task Arithmetic principles to the Federated Learning setting. We hypothesize that by constraining clients to update only the parameters that are least sensitive to the pre-trained knowledge, we can mitigate the adverse effects of client drift caused by data heterogeneity. We utilize a pre-trained DINO ViT-S/16 model and evaluate our method on the CIFAR-100 dataset under various non-IID conditions.

Our key contributions are:
\begin{itemize}
    \item We implement a federated framework using a pre-trained Vision Transformer backbone to study the effects of data heterogeneity.
    \item We integrate a Sparse Fine-Tuning mechanism (SparseSGDM) into the FL loop, masking gradients based on Fisher Information sensitivity to preserve shared knowledge.
    \item We provide a comparative analysis between Centralized training, standard FedAvg, and our Sparse FL approach across different degrees of data heterogeneity.
\end{itemize}