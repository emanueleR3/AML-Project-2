{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Milestone M3 — Centralized Baseline\n",
                "\n",
                "**Goal**: Obtain a solid and reproducible centralized baseline for DINO ViT-S/16 on CIFAR-100.\n",
                "\n",
                "## Targets\n",
                "- Training loop using `src.train`\n",
                "- Hyperparameter sanity checks\n",
                "- Save best checkpoint and metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Import utilities and data functions\n",
                "from src.utils import set_seed, get_device, ensure_dir, save_checkpoint, save_metrics_json, count_parameters, AverageMeter, accuracy\n",
                "from src.data import load_cifar100, create_dataloader\n",
                "from src.model import build_model\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "01a74c8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
                "    \"\"\"\n",
                "    Trains the model for one epoch.\n",
                "    \"\"\"\n",
                "    model.train()\n",
                "    loss_meter = AverageMeter()\n",
                "    acc_meter = AverageMeter()\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Train', leave=False)\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        acc1 = accuracy(outputs, labels, topk=(1,))[0]\n",
                "        loss_meter.update(loss.item(), images.size(0))\n",
                "        acc_meter.update(acc1.item(), images.size(0))\n",
                "        \n",
                "        pbar.set_postfix(loss=f'{loss_meter.avg:.4f}', acc=f'{acc_meter.avg:.2f}%')\n",
                "    \n",
                "    return loss_meter.avg, acc_meter.avg\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def evaluate(model, loader, criterion, device):\n",
                "    \"\"\"\n",
                "    Evaluates the model on the given loader.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    loss_meter = AverageMeter()\n",
                "    acc_meter = AverageMeter()\n",
                "    \n",
                "    for images, labels in tqdm(loader, desc='Eval', leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        acc1 = accuracy(outputs, labels, topk=(1,))[0]\n",
                "        loss_meter.update(loss.item(), images.size(0))\n",
                "        acc_meter.update(acc1.item(), images.size(0))\n",
                "    \n",
                "    return loss_meter.avg, acc_meter.avg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n",
                        "Seed: 42\n",
                        "\n",
                        "Directories created:\n",
                        "  Checkpoints: ./outputs/checkpoints\n",
                        "  Logs: ./outputs/logs/central_baseline\n",
                        "  Figures: ./outputs/figures\n"
                    ]
                }
            ],
            "source": [
                "# Config for the Central Baseline\n",
                "config = {\n",
                "    'exp_name': 'central_baseline',\n",
                "    'seed': 42,\n",
                "    'data_dir': './data',\n",
                "    'output_dir': './outputs',\n",
                "    \n",
                "    # Model\n",
                "    'model_name': 'dino_vits16',\n",
                "    'num_classes': 100,\n",
                "    'freeze_policy': 'head_only',  # 'head_only' | 'finetune_all'\n",
                "    'dropout': 0.0,\n",
                "    \n",
                "    # Training\n",
                "    'epochs': 15,          # 10-20 is usually enough for linear probe\n",
                "    'batch_size': 64,\n",
                "    'lr': 1e-3,\n",
                "    'weight_decay': 1e-4,\n",
                "    'num_workers': 0,      # Set to 0 for stability unless you have a good GPU setup\n",
                "    'device': None         # Will be set automatically\n",
                "}\n",
                "\n",
                "# Set seed for reproducibility\n",
                "set_seed(config['seed'])\n",
                "\n",
                "# Get device\n",
                "device = get_device()\n",
                "config['device'] = device\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Seed: {config['seed']}\")\n",
                "\n",
                "# Setup Directories\n",
                "checkpoint_dir = os.path.join(config['output_dir'], 'checkpoints')\n",
                "log_dir = os.path.join(config['output_dir'], 'logs', config['exp_name'])\n",
                "figures_dir = os.path.join(config['output_dir'], 'figures')\n",
                "\n",
                "ensure_dir(checkpoint_dir)\n",
                "ensure_dir(log_dir)\n",
                "ensure_dir(figures_dir)\n",
                "\n",
                "print(f\"\\nDirectories created:\")\n",
                "print(f\"  Checkpoints: {checkpoint_dir}\")\n",
                "print(f\"  Logs: {log_dir}\")\n",
                "print(f\"  Figures: {figures_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data (CIFAR-100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading CIFAR-100...\n",
                        "\n",
                        "Dataset sizes:\n",
                        "  Train: 45000 samples\n",
                        "  Val:   5000 samples\n",
                        "  Test:  10000 samples\n",
                        "\n",
                        "Dataloaders created:\n",
                        "  Train batches per epoch: 703\n",
                        "  Val batches: 79\n",
                        "  Test batches: 157\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading CIFAR-100...\")\n",
                "# Uses DINO transforms (224x224)\n",
                "train_trainval, test_dataset = load_cifar100(data_dir=config['data_dir'], image_size=224)\n",
                "\n",
                "# Split Train into Train (90%) and Val (10%)\n",
                "train_size = int(0.9 * len(train_trainval))\n",
                "val_size = len(train_trainval) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(\n",
                "    train_trainval, \n",
                "    [train_size, val_size], \n",
                "    generator=torch.Generator().manual_seed(config['seed'])\n",
                ")\n",
                "\n",
                "print(f\"\\nDataset sizes:\")\n",
                "print(f\"  Train: {len(train_dataset)} samples\")\n",
                "print(f\"  Val:   {len(val_dataset)} samples\")\n",
                "print(f\"  Test:  {len(test_dataset)} samples\")\n",
                "\n",
                "# Create dataloaders\n",
                "train_loader = create_dataloader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
                "val_loader = create_dataloader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
                "test_loader = create_dataloader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
                "\n",
                "print(f\"\\nDataloaders created:\")\n",
                "print(f\"  Train batches per epoch: {len(train_loader)}\")\n",
                "print(f\"  Val batches: {len(val_loader)}\")\n",
                "print(f\"  Test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: dino_vits16\n",
                        "  Total parameters: 21,704,164\n",
                        "  Trainable parameters: 38,500\n",
                        "  Trainable %: 0.18%\n",
                        "\n",
                        "Optimizer: AdamW (lr=0.001, wd=0.0001)\n",
                        "Scheduler: CosineAnnealingLR (T_max=15)\n"
                    ]
                }
            ],
            "source": [
                "# Build model\n",
                "model = build_model(config)\n",
                "model.to(device)\n",
                "\n",
                "# Count parameters\n",
                "total_params = count_parameters(model, trainable_only=False)\n",
                "trainable_params = count_parameters(model, trainable_only=True)\n",
                "\n",
                "print(f\"Model: {config['model_name']}\")\n",
                "print(f\"  Total parameters: {total_params:,}\")\n",
                "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
                "\n",
                "# Optimizer + Scheduler\n",
                "optimizer = optim.AdamW(model.get_trainable_params(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "print(f\"\\nOptimizer: AdamW (lr={config['lr']}, wd={config['weight_decay']})\")\n",
                "print(f\"Scheduler: CosineAnnealingLR (T_max={config['epochs']})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "Starting training for 15 epochs...\n",
                        "==================================================\n",
                        "\n",
                        "Epoch 1/15\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Train:   0%|          | 0/703 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
                        "  warnings.warn(warn_msg)\n",
                        "Train:   1%|          | 5/703 [01:14<2:42:25, 13.96s/it, acc=1.25%, loss=7.4937]"
                    ]
                }
            ],
            "source": [
                "# Training loop\n",
                "best_acc = 0.0\n",
                "history = {\n",
                "    'train_loss': [], 'train_acc': [],\n",
                "    'val_loss': [], 'val_acc': []\n",
                "}\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"Starting training for {config['epochs']} epochs...\")\n",
                "print(f\"{'='*50}\\n\")\n",
                "\n",
                "for epoch in range(1, config['epochs'] + 1):\n",
                "    print(f\"Epoch {epoch}/{config['epochs']}\")\n",
                "    \n",
                "    # Train\n",
                "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
                "    \n",
                "    # Validation\n",
                "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
                "    \n",
                "    # Step scheduler\n",
                "    scheduler.step()\n",
                "    \n",
                "    # Logging\n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['train_acc'].append(train_acc)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
                "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    # Save best checkpoint\n",
                "    is_best = val_acc > best_acc\n",
                "    if is_best:\n",
                "        best_acc = val_acc\n",
                "        save_checkpoint({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'best_acc': best_acc,\n",
                "            'config': config,\n",
                "        }, filepath=os.path.join(checkpoint_dir, 'central_best.pt'))\n",
                "        print(f\"  ✓ New best! Saved checkpoint (acc={val_acc:.2f}%)\")\n",
                "    \n",
                "    print()\n",
                "\n",
                "# Save metrics\n",
                "save_metrics_json(os.path.join(log_dir, 'metrics.json'), history)\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Training complete! Best Val Acc: {best_acc:.2f}%\")\n",
                "print(f\"{'='*50}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Evaluation on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "ckpt = torch.load(os.path.join(checkpoint_dir, 'central_best.pt'), map_location=device)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "\n",
                "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"Final Test Results\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Test Loss: {test_loss:.4f}\")\n",
                "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
                "print(f\"Best Val Accuracy: {ckpt['best_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Plotting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "epochs_range = range(1, len(history['train_loss']) + 1)\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Loss plot\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
                "plt.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
                "plt.title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
                "plt.xlabel('Epoch', fontsize=10)\n",
                "plt.ylabel('Loss', fontsize=10)\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy plot\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
                "plt.plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
                "plt.title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
                "plt.xlabel('Epoch', fontsize=10)\n",
                "plt.ylabel('Accuracy (%)', fontsize=10)\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "figure_path = os.path.join(figures_dir, 'central_training_curves.png')\n",
                "plt.savefig(figure_path, dpi=150, bbox_inches='tight')\n",
                "print(f\"Training curves saved to: {figure_path}\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
