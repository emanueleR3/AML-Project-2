{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Setup & Test - DINO + CIFAR-100\n",
                "\n",
                "This notebook:\n",
                "1. Tests the refactored `src` modules\n",
                "2. Loads **DINO ViT-S/16** using `src.model`\n",
                "\n",
                "3. Loads **CIFAR-100** using `src.data`4. Verifies everything works"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2c3d4e5",
            "metadata": {},
            "source": [
                "## 1. Install Required Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b56e5c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/emanueleR3/AML-Project-2.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "c3d4e5f6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
                        "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
                        "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.6/414.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m145.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m177.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages (run if needed)\n",
                "!pip install -q -r AML-Project-2/requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0cdeeee8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('AML-Project-2')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d4e5f6g7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.0+cpu\n",
                        "Torchvision version: 0.24.0+cpu\n",
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import torch\n",
                "import torchvision\n",
                "\n",
                "from src.utils import set_seed, get_device, count_parameters\n",
                "\n",
                "from src.data import load_cifar100, create_dataloader, partition_iid, partition_non_iid\n",
                "\n",
                "from src.model import DINOClassifier, load_dino_backbone\n",
                "\n",
                "# Set seed for reproducibility\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "\n",
                "print(f\"Torchvision version: {torchvision.__version__}\")\n",
                "\n",
                "device = get_device()\n",
                "\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5f6g7h8",
            "metadata": {},
            "source": [
                "## 2. Download DINO ViT-S/16 Model\n",
                "\n",
                "Loading the pretrained DINO ViT-S/16 model from the official Facebook Research GitHub repository:\n",
                "https://github.com/facebookresearch/dino"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "f6g7h8i9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading DINO ViT-S/16 model...\n",
                        "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
                        "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 82.7M/82.7M [00:00<00:00, 507MB/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "DINO Classifier loaded successfully!\n",
                        "Backbone frozen: True\n",
                        "Output classes: 100\n"
                    ]
                }
            ],
            "source": [
                "# Load DINO ViT-S/16 using refactored model.py\n",
                "print(\"Loading DINO ViT-S/16 model...\")\n",
                "\n",
                "model = DINOClassifier(model_name='dino_vits16', num_classes=100, device=device)\n",
                "model.eval()\n",
                "\n",
                "print(\"\\nDINO Classifier loaded successfully!\")\n",
                "print(f\"Backbone frozen: {model.freeze_backbone}\")\n",
                "print(f\"Output classes: 100\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "g7h8i9j0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total parameters: 21,704,164\n",
                        "Trainable parameters: 38,500\n",
                        "Trainable %: 0.18%\n"
                    ]
                }
            ],
            "source": [
                "# Model information using refactored utils\n",
                "total_params = count_parameters(model, trainable_only=False)\n",
                "trainable_params = count_parameters(model, trainable_only=True)\n",
                "\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "h8i9j0k1",
            "metadata": {},
            "source": [
                "## 3. Download CIFAR-100 Dataset\n",
                "\n",
                "Downloading the CIFAR-100 dataset using torchvision. The dataset contains 60,000 32x32 color images in 100 classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "i9j0k1l2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data directory: /content/data\n"
                    ]
                }
            ],
            "source": [
                "# Define data directory\n",
                "data_dir = './data'\n",
                "print(f\"Data directory: {os.path.abspath(data_dir)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "j0k1l2m3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading CIFAR-100 dataset...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 169M/169M [00:03<00:00, 48.0MB/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training set: 50000 images\n",
                        "Test set: 10000 images\n",
                        "Image size after transform: 224x224 (DINO input size)\n"
                    ]
                }
            ],
            "source": [
                "# Load CIFAR-100 using refactored data.py (with DINO transforms)\n",
                "print(\"Loading CIFAR-100 dataset...\")\n",
                "train_dataset, test_dataset = load_cifar100(data_dir=data_dir, image_size=224)\n",
                "\n",
                "print(f\"Training set: {len(train_dataset)} images\")\n",
                "print(f\"Test set: {len(test_dataset)} images\")\n",
                "print(f\"Image size after transform: 224x224 (DINO input size)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "k1l2m3n4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "CIFAR-100 Dataset Information\n",
                        "==================================================\n",
                        "Number of classes: 100\n",
                        "Original size: 32x32x3\n",
                        "Transformed size: 224x224x3 (for DINO)\n",
                        "Training samples: 50000\n",
                        "Test samples: 10000\n",
                        "\n",
                        "Sample classes: ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle']...\n"
                    ]
                }
            ],
            "source": [
                "# Display dataset information\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"CIFAR-100 Dataset Information\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
                "print(f\"Original size: 32x32x3\")\n",
                "print(f\"Transformed size: 224x224x3 (for DINO)\")\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "print(f\"\\nSample classes: {train_dataset.classes[:10]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_md",
            "metadata": {},
            "source": [
                "## 4. Test Forward Pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "33e11e35",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
                        "  warnings.warn(warn_msg)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch shape: torch.Size([8, 3, 224, 224])\n",
                        "Labels: tensor([49, 33, 72, 51, 71, 92, 15, 14])\n",
                        "\n",
                        "Output shape: torch.Size([8, 100])\n",
                        "Output range: [-8.16, 7.47]\n",
                        "\n",
                        "✓ Forward pass successful!\n"
                    ]
                }
            ],
            "source": [
                "# Test model with a batch\n",
                "test_loader = create_dataloader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
                "images, labels = next(iter(test_loader))\n",
                "\n",
                "print(f\"Batch shape: {images.shape}\")\n",
                "print(f\"Labels: {labels}\")\n",
                "\n",
                "# Forward pass\n",
                "model.to(device)\n",
                "images = images.to(device)\n",
                "with torch.no_grad():\n",
                "    outputs = model(images)\n",
                "\n",
                "print(f\"\\nOutput shape: {outputs.shape}\")\n",
                "print(f\"Output range: [{outputs.min():.2f}, {outputs.max():.2f}]\")\n",
                "print(\"\\n✓ Forward pass successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d46376bc",
            "metadata": {},
            "source": [
                "## 5. Test FL Partitioning (M1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "12e37b8a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing IID partitioning...\n",
                        "Number of clients: 10\n",
                        "Client 0: 5000 samples\n",
                        "Client 1: 5000 samples\n",
                        "Client 2: 5000 samples\n",
                        "\n",
                        "Testing non-IID partitioning...\n",
                        "Client 0: 5000 samples, 10 unique classes\n",
                        "Client 1: 5000 samples, 10 unique classes\n",
                        "Client 2: 5000 samples, 10 unique classes\n",
                        "\n",
                        "✓ FL partitioning works!\n"
                    ]
                }
            ],
            "source": [
                "# Test IID partitioning\n",
                "print(\"Testing IID partitioning...\")\n",
                "num_clients = 10\n",
                "client_datasets_iid = partition_iid(train_dataset, num_clients=num_clients, seed=42)\n",
                "\n",
                "print(f\"Number of clients: {num_clients}\")\n",
                "for i in range(min(3, num_clients)):\n",
                "    print(f\"Client {i}: {len(client_datasets_iid[i])} samples\")\n",
                "\n",
                "# Test non-IID partitioning\n",
                "print(\"\\nTesting non-IID partitioning...\")\n",
                "num_classes_per_client = 10\n",
                "client_datasets_noniid = partition_non_iid(\n",
                "    train_dataset, \n",
                "    num_clients=num_clients, \n",
                "    num_classes_per_client=num_classes_per_client,\n",
                "    seed=42\n",
                ")\n",
                "\n",
                "for i in range(min(3, num_clients)):\n",
                "    # Get labels for this client\n",
                "    subset = client_datasets_noniid[i]\n",
                "    client_labels = [train_dataset.targets[idx] for idx in subset.indices]\n",
                "    unique_classes = len(set(client_labels))\n",
                "    print(f\"Client {i}: {len(subset)} samples, {unique_classes} unique classes\")\n",
                "\n",
                "print(\"\\n✓ FL partitioning works!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
