{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b84e0",
   "metadata": {},
   "source": [
    "# AML Project 2: Sparse Federated Learning \n",
    "\n",
    "- Mask Calibration (Fisher Information)\n",
    "- SparseSGDM Optimizer Verification\n",
    "- Sparse FedAvg Training (with IID/Non-IID and various Mask Rules)\n",
    "\n",
    "**Prerequisites:**\n",
    "- This notebook assumes **(Central Baseline)** has been run and a checkpoint `central_baseline.pt` is available in the outputs directory.\n",
    "- Alternatively, it can run with a fresh model (but mask calibration will be random/less effective).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If running on Kaggle, clone repo if not present\n",
    "if not os.path.exists('AML-Project-2') and not os.path.exists('src'):\n",
    "    !git clone https://github.com/emanueleR3/AML-Project-2.git\n",
    "    %cd AML-Project-2\n",
    "\n",
    "# Install requirements\n",
    "!pip install -r requirements.txt\n",
    "!pip install torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils import set_seed, get_device, ensure_dir, save_checkpoint, load_checkpoint, save_metrics_json, count_parameters, AverageMeter\n",
    "from src.data import load_cifar100, create_dataloader, partition_iid, partition_non_iid\n",
    "from src.model import build_model\n",
    "from src.train import evaluate\n",
    "from src.optim import SparseSGDM\n",
    "from src.sparse_fedavg import client_update_sparse, run_fedavg_sparse_round\n",
    "from src.masking import compute_sensitivity_scores, create_mask, save_mask, load_mask, get_mask_sparsity\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "# Setup output dirs\n",
    "OUTPUT_DIR = 'outputs'\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Global Config\n",
    "config = {\n",
    "    'model_name': 'dino_vits16',\n",
    "    'num_classes': 100,\n",
    "    'freeze_policy': 'head_only',\n",
    "    'dropout': 0.1,\n",
    "    'device': device,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We load CIFAR-100 once and reuse it for creating partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_full, test_data = load_cifar100(data_dir='./data', download=True)\n",
    "\n",
    "# Split Train/Val (80/20)\n",
    "train_size = int(0.8 * len(train_full))\n",
    "val_size = len(train_full) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(\n",
    "    train_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = create_dataloader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = 'output/main/central_baseline.pt'\n",
    "\n",
    "model = build_model(config)\n",
    "model.to(device)\n",
    "\n",
    "if os.path.exists(baseline_path):\n",
    "    print(f\"Loading baseline from {baseline_path}...\")\n",
    "    ckpt = torch.load(baseline_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "else:\n",
    "    print(\"Warning: central_baseline.pt not found. Using random initialization for calibration (suboptimal).\")\n",
    "\n",
    "calib_loader = create_dataloader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Computing sensitivity scores...\")\n",
    "scores = compute_sensitivity_scores(model, calib_loader, device, num_batches=100) \n",
    "\n",
    "sparsity_ratio = 0.8\n",
    "\n",
    "rules = ['least_sensitive', 'random', 'highest_magnitude']\n",
    "masks = {}\n",
    "\n",
    "for rule in rules:\n",
    "    mask = create_mask(scores, model, sparsity_ratio=sparsity_ratio, rule=rule)\n",
    "    masks[rule] = mask\n",
    "    \n",
    "    path = os.path.join(OUTPUT_DIR, f'mask_{rule}_0.8.pt')\n",
    "    save_mask(mask, path)\n",
    "    \n",
    "    active_ratio = get_mask_sparsity(mask)\n",
    "    print(f\"Rule: {rule} | Active params: {active_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseSGDM Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing SparseSGDM ---\")\n",
    "dummy_model = nn.Linear(10, 1)\n",
    "\n",
    "# First 5 active, last 5 masked (0)\n",
    "dummy_mask = {\n",
    "    'weight': torch.cat([torch.ones(1, 5), torch.zeros(1, 5)], dim=1),\n",
    "    'bias': torch.ones(1)\n",
    "}\n",
    "\n",
    "optimizer = SparseSGDM(dummy_model.parameters(), lr=0.1, mask=dummy_mask)\n",
    "\n",
    "initial_weight = dummy_model.weight.data.clone()\n",
    "\n",
    "# Artificial gradient\n",
    "dummy_model.weight.grad = torch.ones_like(dummy_model.weight)\n",
    "dummy_model.bias.grad = torch.ones_like(dummy_model.bias)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "updated_weight = dummy_model.weight.data\n",
    "diff = (updated_weight - initial_weight).abs()\n",
    "\n",
    "print(f\"Initial Weights (first 5 active): {initial_weight[0, :5]}\")\n",
    "print(f\"Initial Weights (last 5 masked):  {initial_weight[0, 5:]}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Updated Weights (first 5 active): {updated_weight[0, :5]}\")\n",
    "print(f\"Updated Weights (last 5 masked):  {updated_weight[0, 5:]}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Difference (first 5): {diff[0, :5]}\")\n",
    "print(f\"Difference (last 5):  {diff[0, 5:]}\")\n",
    "\n",
    "# Verification Logic\n",
    "if torch.all(diff[0, 5:] == 0) and torch.any(diff[0, :5] > 0):\n",
    "    print(\"\\n✅ PASS: Masked parameters remained unchanged, active parameters updated.\")\n",
    "else:\n",
    "    print(\"\\n❌ FAIL: Optimization behavior incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse FedAvg Experiments (Real Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Training Loop Function\n",
    "def run_experiment(exp_name, is_iid, nc, mask_rule, num_rounds=100):\n",
    "    \n",
    "    num_clients = 100 \n",
    "    if is_iid:\n",
    "        client_datasets = partition_iid(train_data, num_clients)\n",
    "    else:\n",
    "        client_datasets = partition_non_iid(train_data, num_clients, num_classes_per_client=nc)\n",
    "        \n",
    "    client_loaders = [create_dataloader(ds, batch_size=32, shuffle=True) for ds in client_datasets]\n",
    "    \n",
    "    model = build_model(config)\n",
    "    model.to(device)\n",
    "    \n",
    "    mask = masks[mask_rule] \n",
    "    \n",
    "    clients_per_round = 0.1 \n",
    "    m = max(1, int(num_clients * clients_per_round))\n",
    "    \n",
    "    history = {'round': [], 'val_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for r in range(1, num_rounds + 1):\n",
    "        selected_clients = np.random.choice(num_clients, m, replace=False)\n",
    "        \n",
    "        loss, acc = run_fedavg_sparse_round(\n",
    "            model, client_loaders, selected_clients,\n",
    "            lr=0.01, weight_decay=1e-4, device=device,\n",
    "            local_steps=4, \n",
    "            mask=mask\n",
    "        )\n",
    "        \n",
    "         # Eval every 2 rounds\n",
    "        if r % 2 == 0 or r == num_rounds:\n",
    "            val_loss, val_acc = evaluate(model, val_loader, nn.CrossEntropyLoss(), device, show_progress=False)\n",
    "            test_loss, test_acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), device, show_progress=False)\n",
    "            print(f\"Round {r}: TrainAcc={acc:.2f}%, ValAcc={val_acc:.2f}%, TestAcc={test_acc:.2f}%\")\n",
    "            \n",
    "            history['round'].append(r)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "        else:\n",
    "            print(f\"Round {r}: TrainAcc={acc:.2f}%\")\n",
    "        \n",
    "    save_metrics_json(os.path.join(OUTPUT_DIR, f'{exp_name}_metrics.json'), history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: IID + Least Sensitive\n",
    "hist_iid = run_experiment('exp_iid_ls', is_iid=True, nc=None, mask_rule='least_sensitive', num_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Non-IID (Nc=1) + Least Sensitive\n",
    "hist_niid_ls = run_experiment('exp_niid_ls', is_iid=False, nc=1, mask_rule='least_sensitive', num_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Non-IID (Nc=1) + Random Mask (Extension)\n",
    "hist_niid_rnd = run_experiment('exp_niid_rnd', is_iid=False, nc=1, mask_rule='random', num_rounds=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
