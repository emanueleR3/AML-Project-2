{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# AML Project 2: Protocol Step 4 - Scaled Non-IID Sweep\n",
                "\n",
                "This notebook runs the Non-IID experiments with scaled rounds to ensure constant computation, using the **Backbone Fine-tuning Protocol**.\n",
                "- **Pre-requisite:** `pretrained_head.pt` (Protocol Step 1)\n",
                "- **Backbone:** Unfrozen (`finetune_all`)\n",
                "- **Classifier:** Frozen (`freeze_head=True`)\n",
                "- **Base Rounds:** 300 (for J=4)\n",
                "- **Scaling Logic:** Rounds = (4 * 300) / J"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository & Install Dependencies\n",
                "!git clone https://github.com/emanueleR3/AML-Project-2.git\n",
                "%cd AML-Project-2\n",
                "!pip install -r requirements.txt\n",
                "!pip install torch torchvision numpy matplotlib tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports & Setup\n",
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "from src.utils import set_seed, get_device, ensure_dir, save_metrics_json\n",
                "from src.data import load_cifar100, create_dataloader, partition_non_iid\n",
                "from src.model import build_model\n",
                "from src.train import evaluate\n",
                "from src.fedavg import run_fedavg\n",
                "\n",
                "sys.path.append('.')\n",
                "\n",
                "# Setup output dirs\n",
                "OUTPUT_DIR = 'output/scaled'\n",
                "ensure_dir(OUTPUT_DIR)\n",
                "device = get_device()\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "print(\"Loading CIFAR-100...\")\n",
                "train_trainval, test_dataset = load_cifar100(data_dir='./data', image_size=224, download=True)\n",
                "\n",
                "# Split Train/Val (Consistent with other notebooks)\n",
                "train_size = int(0.9 * len(train_trainval))\n",
                "val_size = len(train_trainval) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(\n",
                "    train_trainval, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
                ")\n",
                "\n",
                "val_loader = create_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
                "test_loader = create_dataloader(test_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base Configuration for Non-IID Sweep\n",
                "base_config = {\n",
                "    'num_clients': 100,\n",
                "    'clients_per_round': 0.1,\n",
                "    'local_steps': 4,\n",
                "    'num_rounds': 300,\n",
                "    'batch_size': 64,\n",
                "    'lr': 1e-4,  # Lower LR for backbone\n",
                "    'weight_decay': 1e-4,\n",
                "    'seed': 42,\n",
                "    'eval_freq': 10\n",
                "}\n",
                "\n",
                "model_config = {\n",
                "    'model_name': 'dino_vits16',\n",
                "    'num_classes': 100,\n",
                "    'freeze_policy': 'finetune_all', # Unfreeze backbone\n",
                "    'freeze_head': True,             # Freeze classifier\n",
                "    'dropout': 0.1,\n",
                "    'device': device\n",
                "}\n",
                "\n",
                "# Path to pre-trained head\n",
                "HEAD_PATH = 'output/main/pretrained_head.pt'\n",
                "if not os.path.exists(HEAD_PATH):\n",
                "    raise FileNotFoundError(f\"Pre-trained head not found at {HEAD_PATH}. Run pretrain_head.ipynb first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sweep_logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaled Rounds Logic\n",
                "BASE_J = 4\n",
                "BASE_ROUNDS = 300\n",
                "TOTAL_STEPS = BASE_J * BASE_ROUNDS  # = 1200\n",
                "\n",
                "def get_scaled_rounds(j):\n",
                "    return TOTAL_STEPS // j\n",
                "\n",
                "# ==========================================\n",
                "# SWEEP CONTROL - CONFIGURE EXECUTION HERE\n",
                "# ==========================================\n",
                "RUN_ALL_NC = False  # Set to True to run ALL scenarios\n",
                "TARGET_NC = 50       # Use this logic to split work among team members. Options: 1, 5, 10, 50\n",
                "# ==========================================\n",
                "\n",
                "# Sweep Parameters\n",
                "NC_VALUES_ALL = [1, 5, 10, 50]\n",
                "J_VALUES = [4, 8, 16]\n",
                "\n",
                "if RUN_ALL_NC:\n",
                "    nc_scenarios = NC_VALUES_ALL\n",
                "    print(\"Running FULL sweep for all Nc values.\")\n",
                "else:\n",
                "    nc_scenarios = [TARGET_NC]\n",
                "    print(f\"Running PARTIAL sweep for Nc={TARGET_NC} only.\")\n",
                "\n",
                "\n",
                "print(\"\\nScaled Rounds Configuration:\")\n",
                "for j in J_VALUES:\n",
                "    print(f\"  J={j:2d} → Rounds={get_scaled_rounds(j):3d} | Total Steps={j * get_scaled_rounds(j)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_sweep",
            "metadata": {},
            "outputs": [],
            "source": [
                "for nc in nc_scenarios:\n",
                "    for j in J_VALUES:\n",
                "        scaled_rounds = get_scaled_rounds(j)\n",
                "        exp_name = f'noniid_nc{nc}_j{j}'\n",
                "        print(f\"\\n\" + \"*\"*40)\n",
                "        print(f\"Starting: Nc={nc}, J={j}, Rounds={scaled_rounds}\")\n",
                "        print(f\"{'*'*40}\")\n",
                "        \n",
                "        # 1. Partition Data (Reproducible seed)\n",
                "        client_datasets = partition_non_iid(train_dataset, 100, nc, 42)\n",
                "        client_loaders = [create_dataloader(ds, 64, True, 0) for ds in client_datasets]\n",
                "        \n",
                "        # 2. Configure Experiment\n",
                "        sweep_config = base_config.copy()\n",
                "        sweep_config['local_steps'] = j\n",
                "        sweep_config['num_rounds'] = scaled_rounds\n",
                "        \n",
                "        # 3. Build Fresh Model with Pre-trained Head\n",
                "        model = build_model(model_config)\n",
                "        model.to(device)\n",
                "        \n",
                "        # Load pre-trained head\n",
                "        ckpt = torch.load(HEAD_PATH, map_location=device)\n",
                "        model.load_state_dict(ckpt['model_state_dict'])\n",
                "        \n",
                "        # 4. Run Training\n",
                "        history = run_fedavg(model, client_loaders, val_loader, test_loader, sweep_config, device)\n",
                "        \n",
                "        # 5. Save Results\n",
                "        save_metrics_json(os.path.join(OUTPUT_DIR, f'{exp_name}.json'), history)\n",
                "        print(f\"✓ Final Test Acc: {history['test_acc'][-1]:.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
