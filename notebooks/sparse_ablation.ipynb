{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# AML Project 2: Sparse Ablation Study + Extension\n",
                "\n",
                "This notebook runs all sparse FL ablation studies:\n",
                "\n",
                "**Ablation 1: Calibration Rounds** (fixed 50% sparsity)\n",
                "- Tests: 1, 3, 5, 10 calibration rounds\n",
                "\n",
                "**Ablation 2: Sparsity Ratio** (fixed 3 calibration rounds)  \n",
                "- Tests: 20%, 50%, 90% sparsity\n",
                "\n",
                "**Ablation 3 (Extension): Mask Rule Comparison**\n",
                "- 5 rules tested at 80% sparsity, 3 calibration rounds:\n",
                "  1. **Least Sensitive** - Keep weights with LOW Fisher scores (paper's approach)\n",
                "  2. **Most Sensitive** - Keep weights with HIGH Fisher scores\n",
                "  3. **Lowest Magnitude** - Keep weights with LOW absolute values\n",
                "  4. **Highest Magnitude** - Keep weights with HIGH absolute values (traditional pruning)\n",
                "  5. **Random** - Random selection baseline\n",
                "\n",
                "**Protocol:** Backbone Fine-tuning (`finetune_all` + `freeze_head=True`)  \n",
                "**Rounds:** 50 per experiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository & Install Dependencies\n",
                "!git clone https://github.com/emanueleR3/AML-Project-2.git\n",
                "%cd AML-Project-2\n",
                "!pip install -r requirements.txt\n",
                "!pip install torch torchvision numpy matplotlib tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "\n",
                "from src.utils import set_seed, get_device, ensure_dir, save_metrics_json\n",
                "from src.data import load_cifar100, create_dataloader, partition_iid\n",
                "from src.model import build_model\n",
                "from src.train import evaluate\n",
                "from src.sparse_fedavg import run_fedavg_sparse_round\n",
                "from src.masking import compute_fisher_diagonal, create_mask, get_mask_sparsity\n",
                "\n",
                "sys.path.append('.')\n",
                "\n",
                "OUTPUT_DIR = 'output/sparse_ablation'\n",
                "ensure_dir(OUTPUT_DIR)\n",
                "device = get_device()\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mask_utils",
            "metadata": {},
            "outputs": [],
            "source": [
                "# NOTE: We do NOT apply the mask to the model weights initially (Pruning).\n",
                "# We only mask the gradients during training (Sparse Updates).\n",
                "# This ensures the 'frozen' backbone weights retain their pretrained values.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "train_full, test_data = load_cifar100(data_dir='./data', download=True)\n",
                "train_size = int(0.9 * len(train_full))\n",
                "val_size = len(train_full) - train_size\n",
                "train_data, val_data = torch.utils.data.random_split(\n",
                "    train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
                ")\n",
                "\n",
                "val_loader = create_dataloader(val_data, batch_size=64, shuffle=False)\n",
                "test_loader = create_dataloader(test_data, batch_size=64, shuffle=False)\n",
                "\n",
                "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "NUM_ROUNDS = 50\n",
                "SPARSITY_LEVELS = [0.2, 0.5, 0.9]\n",
                "CALIB_ROUNDS = [1, 3, 5, 10]\n",
                "\n",
                "# Training hyperparams (same as dense baseline)\n",
                "LR = 1e-4\n",
                "WEIGHT_DECAY = 1e-4\n",
                "LOCAL_STEPS = 4\n",
                "NUM_CLIENTS = 100\n",
                "CLIENTS_PER_ROUND = 0.1\n",
                "EVAL_FREQ = 5\n",
                "\n",
                "model_config = {\n",
                "    'model_name': 'dino_vits16',\n",
                "    'num_classes': 100,\n",
                "    'freeze_policy': 'finetune_all',\n",
                "    'freeze_head': True,\n",
                "    'device': device\n",
                "}\n",
                "\n",
                "BASELINE_PATH = 'output/main/pretrained_head.pt'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_ablation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Partition Data (IID for ablation)\n",
                "client_datasets = partition_iid(train_data, NUM_CLIENTS)\n",
                "client_loaders = [create_dataloader(ds, 32, True, 0) for ds in client_datasets]\n",
                "\n",
                "# Load pretrained model state\n",
                "if os.path.exists(BASELINE_PATH):\n",
                "    pretrained_state = torch.load(BASELINE_PATH, map_location=device)['model_state_dict']\n",
                "    print(f\"✓ Loaded pretrained model from {BASELINE_PATH}\")\n",
                "else:\n",
                "    pretrained_state = None\n",
                "    print(\"⚠ No pretrained model found, using random initialization\")\n",
                "\n",
                "\n",
                "def run_sparse_experiment(exp_name, mask, num_rounds=NUM_ROUNDS):\n",
                "    \"\"\"Run sparse FedAvg with SparseSGDM optimizer.\"\"\"\n",
                "    print(f\"\\nRunning: {exp_name}\")\n",
                "    \n",
                "    # Build fresh model\n",
                "    model = build_model(model_config)\n",
                "    model.to(device)\n",
                "    if pretrained_state is not None:\n",
                "        model.load_state_dict(pretrained_state)\n",
                "    \n",
                "    # NOTE: No initial pruning applied here.\n",
                "    print(f\"  Mask active ratio: {get_mask_sparsity(mask):.2%}\")\n",
                "    \n",
                "    m = max(1, int(NUM_CLIENTS * CLIENTS_PER_ROUND))\n",
                "    history = {'round': [], 'train_acc': [], 'val_acc': [], 'test_acc': []}\n",
                "    \n",
                "    for r in range(1, num_rounds + 1):\n",
                "        selected = np.random.choice(NUM_CLIENTS, m, replace=False)\n",
                "        \n",
                "        # Use SparseSGDM via run_fedavg_sparse_round\n",
                "        loss, acc = run_fedavg_sparse_round(\n",
                "            model, client_loaders, selected.tolist(),\n",
                "            lr=LR, weight_decay=WEIGHT_DECAY, device=device,\n",
                "            local_steps=LOCAL_STEPS, mask=mask\n",
                "        )\n",
                "        \n",
                "        # Always save train accuracy\n",
                "        history['round'].append(r)\n",
                "        history['train_acc'].append(acc)\n",
                "        \n",
                "        # Only evaluate val/test periodically (expensive)\n",
                "        if r % EVAL_FREQ == 0 or r == num_rounds:\n",
                "            val_loss, val_acc = evaluate(model, val_loader, nn.CrossEntropyLoss(), device, show_progress=False)\n",
                "            test_loss, test_acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), device, show_progress=False)\n",
                "            print(f\"  Round {r}/{num_rounds} | Train: {acc:.1f}% | Val: {val_acc:.1f}% | Test: {test_acc:.1f}%\")\n",
                "            history['val_acc'].append(val_acc)\n",
                "            history['test_acc'].append(test_acc)\n",
                "        else:\n",
                "            history['val_acc'].append(None)\n",
                "            history['test_acc'].append(None)\n",
                "    \n",
                "    save_metrics_json(os.path.join(OUTPUT_DIR, f'{exp_name}.json'), history)\n",
                "    print(f\"  → Final Test Acc: {history['test_acc'][-1]:.2f}%\")\n",
                "    return history\n",
                "\n",
                "\n",
                "# ============================================\n",
                "# 1. Calibration Rounds Ablation (fixed 50% sparsity)\n",
                "# ============================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ABLATION 1: Calibration Rounds (sparsity=50%)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "calib_results = {}\n",
                "\n",
                "for num_calib in CALIB_ROUNDS:\n",
                "    print(f\"\\n--- Calibration Rounds = {num_calib} ---\")\n",
                "    \n",
                "    # Fresh model for calibration\n",
                "    model = build_model(model_config)\n",
                "    model.to(device)\n",
                "    if pretrained_state is not None:\n",
                "        model.load_state_dict(pretrained_state)\n",
                "    \n",
                "    # Multi-round Fisher calibration\n",
                "    cumulative_fisher = None\n",
                "    m = max(1, int(NUM_CLIENTS * CLIENTS_PER_ROUND))\n",
                "    \n",
                "    for cal_round in range(1, num_calib + 1):\n",
                "        selected = np.random.choice(NUM_CLIENTS, m, replace=False)\n",
                "        round_fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
                "        \n",
                "        for idx in selected:\n",
                "            client_fisher = compute_fisher_diagonal(model, client_loaders[idx], device, num_batches=50)\n",
                "            for n in round_fisher:\n",
                "                if n in client_fisher:\n",
                "                    round_fisher[n] += client_fisher[n]\n",
                "        \n",
                "        for n in round_fisher:\n",
                "            round_fisher[n] /= len(selected)\n",
                "        \n",
                "        if cumulative_fisher is None:\n",
                "            cumulative_fisher = {n: f.clone() for n, f in round_fisher.items()}\n",
                "        else:\n",
                "            for n in cumulative_fisher:\n",
                "                cumulative_fisher[n] += round_fisher[n]\n",
                "    \n",
                "    for n in cumulative_fisher:\n",
                "        cumulative_fisher[n] /= num_calib\n",
                "    \n",
                "    # Create mask with LEAST SENSITIVE rule (keep low Fisher weights)\n",
                "    mask = create_mask(cumulative_fisher, model, sparsity_ratio=0.5, rule='least_sensitive')\n",
                "    \n",
                "    # Train\n",
                "    history = run_sparse_experiment(f'ablation_calib{num_calib}', mask)\n",
                "    calib_results[num_calib] = history['test_acc'][-1]\n",
                "\n",
                "print(\"\\n--- Calibration Rounds Summary ---\")\n",
                "for k, v in calib_results.items():\n",
                "    print(f\"  {k} rounds: {v:.2f}%\")\n",
                "best_calib = max(calib_results, key=calib_results.get)\n",
                "print(f\"  → Best: {best_calib} calibration rounds\")\n",
                "\n",
                "\n",
                "# ============================================\n",
                "# 2. Sparsity Ratio Ablation (fixed 3 calibration rounds)\n",
                "# ============================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ABLATION 2: Sparsity Ratio (calibration=3)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Compute Fisher with 3 calibration rounds\n",
                "model = build_model(model_config)\n",
                "model.to(device)\n",
                "if pretrained_state is not None:\n",
                "    model.load_state_dict(pretrained_state)\n",
                "\n",
                "cumulative_fisher = None\n",
                "m = max(1, int(NUM_CLIENTS * CLIENTS_PER_ROUND))\n",
                "\n",
                "for cal_round in range(1, 4):\n",
                "    selected = np.random.choice(NUM_CLIENTS, m, replace=False)\n",
                "    round_fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
                "    \n",
                "    for idx in selected:\n",
                "        client_fisher = compute_fisher_diagonal(model, client_loaders[idx], device, num_batches=50)\n",
                "        for n in round_fisher:\n",
                "            if n in client_fisher:\n",
                "                round_fisher[n] += client_fisher[n]\n",
                "    \n",
                "    for n in round_fisher:\n",
                "        round_fisher[n] /= len(selected)\n",
                "    \n",
                "    if cumulative_fisher is None:\n",
                "        cumulative_fisher = {n: f.clone() for n, f in round_fisher.items()}\n",
                "    else:\n",
                "        for n in cumulative_fisher:\n",
                "            cumulative_fisher[n] += round_fisher[n]\n",
                "\n",
                "for n in cumulative_fisher:\n",
                "    cumulative_fisher[n] /= 3\n",
                "\n",
                "sparsity_results = {}\n",
                "\n",
                "for sparsity in SPARSITY_LEVELS:\n",
                "    print(f\"\\n--- Sparsity = {sparsity:.0%} ---\")\n",
                "    \n",
                "    # Create mask with LEAST SENSITIVE rule\n",
                "    mask = create_mask(cumulative_fisher, model, sparsity_ratio=sparsity, rule='least_sensitive')\n",
                "    \n",
                "    history = run_sparse_experiment(f'ablation_sparsity{int(sparsity*100)}', mask)\n",
                "    sparsity_results[sparsity] = history['test_acc'][-1]\n",
                "\n",
                "print(\"\\n--- Sparsity Ratio Summary ---\")\n",
                "for k, v in sparsity_results.items():\n",
                "    print(f\"  {k:.0%}: {v:.2f}%\")\n",
                "best_sparsity = max(sparsity_results, key=sparsity_results.get)\n",
                "print(f\"  → Best: {best_sparsity:.0%} sparsity\")\n",
                "\n",
                "\n",
                "# ============================================\n",
                "# 3. EXTENSION: Mask Rule Comparison\n",
                "# ============================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ABLATION 3: Mask Rule Comparison (Extension)\")\n",
                "print(\"  Using best settings: calibration=3, sparsity=80%\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# All 5 mask rules to compare\n",
                "MASK_RULES = [\n",
                "    'least_sensitive',    # Paper's recommended approach\n",
                "    'most_sensitive',     # Opposite: keep high-sensitivity weights\n",
                "    'lowest_magnitude',   # Traditional pruning: remove small weights\n",
                "    'highest_magnitude',  # Opposite: remove large weights\n",
                "    'random'              # Baseline: random selection\n",
                "]\n",
                "\n",
                "# Use the Fisher scores already computed (3 calibration rounds)\n",
                "rule_results = {}\n",
                "\n",
                "for rule in MASK_RULES:\n",
                "    print(f\"\\n--- Mask Rule: {rule.replace('_', ' ').title()} ---\")\n",
                "    \n",
                "    # Create mask with this rule\n",
                "    mask = create_mask(cumulative_fisher, model, sparsity_ratio=0.8, rule=rule)\n",
                "    \n",
                "    # Train\n",
                "    history = run_sparse_experiment(f'ablation_rule_{rule}', mask)\n",
                "    rule_results[rule] = history['test_acc'][-1]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"MASK RULE COMPARISON SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Sort by accuracy\n",
                "sorted_rules = sorted(rule_results.items(), key=lambda x: x[1], reverse=True)\n",
                "for i, (rule, acc) in enumerate(sorted_rules):\n",
                "    marker = \"★ BEST\" if i == 0 else \"\"\n",
                "    print(f\"  {rule.replace('_', ' ').title():20s}: {acc:.2f}% {marker}\")\n",
                "\n",
                "# Key comparison\n",
                "ls_acc = rule_results.get('least_sensitive', 0)\n",
                "rnd_acc = rule_results.get('random', 0)\n",
                "print(f\"\\n  Least Sensitive vs Random: {'+' if ls_acc > rnd_acc else ''}{ls_acc - rnd_acc:.2f} pp\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}