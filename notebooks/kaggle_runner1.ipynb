{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6d422505",
            "metadata": {},
            "source": [
                "# AML Project 2: Federated Learning\n",
                "\n",
                "- Setup & Central Baseline\n",
                "- FedAvg IID\n",
                "- Non-IID Sweep (with Scaled Rounds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9244f9f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository & Install Dependencies\n",
                "!git clone https://github.com/emanueleR3/AML-Project-2.git\n",
                "%cd AML-Project-2\n",
                "!pip install -r requirements.txt\n",
                "!pip install torch torchvision numpy matplotlib tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "862d500d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports & Setup\n",
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "from src.utils import set_seed, get_device, ensure_dir, save_checkpoint, save_metrics_json, count_parameters\n",
                "from src.data import load_cifar100, create_dataloader, partition_iid, partition_non_iid\n",
                "from src.model import build_model\n",
                "from src.train import evaluate, train_one_epoch\n",
                "from src.fedavg import run_fedavg\n",
                "from src.masking import compute_sensitivity_scores, create_mask, save_mask\n",
                "\n",
                "sys.path.append('.')\n",
                "\n",
                "# Setup output dirs\n",
                "OUTPUT_DIR = 'output/main'\n",
                "ensure_dir(OUTPUT_DIR)\n",
                "device = get_device()\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "893e146e",
            "metadata": {},
            "source": [
                "## Setup & Central Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cbbd6f01",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DINO & Data\n",
                "print(\"Loading CIFAR-100...\")\n",
                "train_trainval, test_dataset = load_cifar100(data_dir='./data', image_size=224, download=True)\n",
                "\n",
                "# Split Train/Val\n",
                "train_size = int(0.9 * len(train_trainval))\n",
                "val_size = len(train_trainval) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(\n",
                "    train_trainval, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
                ")\n",
                "\n",
                "# Create loaders\n",
                "val_loader = create_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
                "test_loader = create_dataloader(test_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "print(\"Data loaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eae86f8c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Real Central Baseline\n",
                "print(\"\\nRunning Real Central Baseline...\")\n",
                "config = {\n",
                "    'model_name': 'dino_vits16',\n",
                "    'num_classes': 100,\n",
                "    'freeze_policy': 'head_only',\n",
                "    'dropout': 0.1,\n",
                "    'device': device\n",
                "}\n",
                "\n",
                "model = build_model(config)\n",
                "model.to(device)\n",
                "\n",
                "# Hyperparameters Baseline\n",
                "epochs = 20\n",
                "eval_freq = 2\n",
                "optimizer = torch.optim.AdamW(model.get_trainable_params(), lr=1e-3)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "train_loader = create_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
                "best_acc = 0.0\n",
                "\n",
                "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    loss, acc = train_one_epoch(model, train_loader, optimizer, criterion, device, show_progress=False)\n",
                "    \n",
                "    # Validation logic\n",
                "    current_epoch = epoch + 1\n",
                "    if current_epoch % eval_freq == 0 or current_epoch == epochs: \n",
                "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, show_progress=False)\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            save_checkpoint({'model_state_dict': model.state_dict()}, os.path.join(OUTPUT_DIR, 'central_baseline.pt'))\n",
                "            \n",
                "        print(f\"Epoch {current_epoch}/{epochs} | Train Acc: {acc:.2f}% | Val Acc: {val_acc:.2f}% | Best: {best_acc:.2f}%\")\n",
                "        \n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "\n",
                "    else:\n",
                "        print(f\"Epoch {current_epoch}/{epochs} | Train Acc: {acc:.2f}% | (Skipping Eval)\")\n",
                "    \n",
                "    scheduler.step()\n",
                "    \n",
                "    history['train_loss'].append(loss)\n",
                "    history['train_acc'].append(acc)\n",
                "\n",
                "print(f\"Baseline finished. Best Val Acc: {best_acc:.2f}%\")\n",
                "\n",
                "save_metrics_json(os.path.join(OUTPUT_DIR, 'central_baseline_metrics.json'), history)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7830b4a5",
            "metadata": {},
            "source": [
                "## FedAvg IID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b8aebcb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Config\n",
                "iid_config = {\n",
                "    'num_clients': 100,\n",
                "    'clients_per_round': 0.1,\n",
                "    'local_steps': 4,\n",
                "    'num_rounds': 300,\n",
                "    'batch_size': 64,\n",
                "    'lr': 0.001,\n",
                "    'weight_decay': 1e-4,\n",
                "    'seed': 42,\n",
                "    'eval_freq': 10\n",
                "}\n",
                "\n",
                "# Partition IID\n",
                "print(\"Partitioning IID...\")\n",
                "client_datasets = partition_iid(train_dataset, iid_config['num_clients'], iid_config['seed'])\n",
                "client_loaders = [create_dataloader(ds, iid_config['batch_size'], True, 0) for ds in client_datasets]\n",
                "\n",
                "# Run\n",
                "model = build_model(config)\n",
                "model.to(device)\n",
                "\n",
                "print(\"Starting FedAvg IID...\")\n",
                "history = run_fedavg(model, client_loaders, val_loader, test_loader, iid_config, device)\n",
                "\n",
                "# Save\n",
                "save_metrics_json(os.path.join(OUTPUT_DIR, 'fedavg_iid_metrics.json'), history)\n",
                "save_checkpoint({'model_state_dict': model.state_dict()}, os.path.join(OUTPUT_DIR, 'fedavg_iid_best.pt'))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "162914c3",
            "metadata": {},
            "source": [
                "## Non-IID Sweep (Scaled Rounds)\n",
                "\n",
                "When increasing local steps J, we scale rounds inversely to keep total computation constant:\n",
                "- J=4 → 100 rounds (baseline)\n",
                "- J=8 → 50 rounds  \n",
                "- J=16 → 25 rounds\n",
                "\n",
                "Total local steps per sampled client = J × Rounds = 400"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0937fc6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaled Rounds Configuration\n",
                "BASE_J = 4\n",
                "BASE_ROUNDS = 100\n",
                "TOTAL_STEPS = BASE_J * BASE_ROUNDS  # = 400\n",
                "\n",
                "def get_scaled_rounds(j):\n",
                "    \"\"\"Calculate rounds to keep total computation constant.\"\"\"\n",
                "    return TOTAL_STEPS // j\n",
                "\n",
                "# Sweep Params\n",
                "NC_VALUES = [1, 5, 10, 50] \n",
                "J_VALUES = [4, 8, 16]      \n",
                "\n",
                "print(\"Scaled Rounds Configuration:\")\n",
                "for j in J_VALUES:\n",
                "    print(f\"  J={j:2d} → Rounds={get_scaled_rounds(j):3d} | Total Steps={j * get_scaled_rounds(j)}\")\n",
                "\n",
                "for nc in NC_VALUES:\n",
                "    for j in J_VALUES:\n",
                "        scaled_rounds = get_scaled_rounds(j)\n",
                "        print(f\"\\n--- Runs Non-IID: Nc={nc}, J={j}, Rounds={scaled_rounds} ---\")\n",
                "        \n",
                "        # Partition \n",
                "        client_datasets = partition_non_iid(train_dataset, 100, nc, 42)\n",
                "        client_loaders = [create_dataloader(ds, 64, True, 0) for ds in client_datasets]\n",
                "        \n",
                "        # Config with scaled rounds\n",
                "        sweep_config = iid_config.copy()\n",
                "        sweep_config['local_steps'] = j\n",
                "        sweep_config['num_rounds'] = scaled_rounds\n",
                "        \n",
                "        # Run\n",
                "        model = build_model(config)\n",
                "        model.to(device)\n",
                "        history = run_fedavg(model, client_loaders, val_loader, test_loader, sweep_config, device)\n",
                "        \n",
                "        # Save\n",
                "        save_metrics_json(os.path.join(OUTPUT_DIR, f'noniid_nc{nc}_j{j}.json'), history)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
