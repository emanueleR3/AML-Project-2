{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25dd5a3",
   "metadata": {},
   "source": [
    "# Milestone M2 — DINO ViT-S/16 + head CIFAR-100\n",
    "\n",
    "Goal (from `docs/project_tasks.md`):\n",
    "- Build a model: **DINO backbone** + **linear head** for 100 classes\n",
    "- Support **freeze policies**:\n",
    "  - `head_only` (backbone frozen)\n",
    "  - `finetune_all` (everything trainable)\n",
    "  - `last_blocks_only` (optional: only last N blocks trainable)\n",
    "- Provide helpers: `get_trainable_params(model)` and `count_params(model)`\n",
    "\n",
    "Note: keep this notebook **code-only** here (don’t run if imports are broken in your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# If running from notebooks/, add project root to PYTHONPATH\n",
    "sys.path.append('AML-Project-2')\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.utils import get_device\n",
    "from src.model import build_model, count_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302954a6",
   "metadata": {},
   "source": [
    "## 1) Build model from config\n",
    "\n",
    "This matches the deliverable: `build_model(config)` returns a ready-to-train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "config = {\n",
    "    'model_name': 'dino_vits16',\n",
    "    'num_classes': 100,\n",
    "    'dropout': 0.1,\n",
    "    # freeze_policy: 'head_only' | 'finetune_all' | 'last_blocks_only'\n",
    "    'freeze_policy': 'head_only',\n",
    "    # used only for 'last_blocks_only'\n",
    "    'last_n_blocks': 2,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "model = build_model(config)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b3380",
   "metadata": {},
   "source": [
    "## 2) Count params (logging helper)\n",
    "\n",
    "Milestone M2 requests `count_params(model)` for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e191d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = count_params(model, trainable_only=False)\n",
    "trainable = count_params(model, trainable_only=True)\n",
    "print('Total params:', total)\n",
    "print('Trainable params:', trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d1be1",
   "metadata": {},
   "source": [
    "## 3) Stop condition checks (forward + single backward)\n",
    "\n",
    "M2 stop condition:\n",
    "- Forward on dummy batch returns logits shape `[B, 100]`\n",
    "- A single training step (loss + backward) does not error\n",
    "\n",
    "Below is the code to run once your environment imports work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy forward pass\n",
    "B = 4\n",
    "x = torch.randn(B, 3, 224, 224, device=device)\n",
    "logits = model(x)\n",
    "print('logits shape:', tuple(logits.shape))  # expected: (B, 100)\n",
    "\n",
    "# Single training step (example)\n",
    "# y = torch.randint(low=0, high=100, size=(B,), device=device)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.get_trainable_params(), lr=0.01, momentum=0.9)\n",
    "# loss = loss_fn(logits, y)\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# print('loss:', float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12615e69",
   "metadata": {},
   "source": [
    "## 4) Freeze policy quick examples\n",
    "\n",
    "Switching `freeze_policy` changes which backbone params are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a67084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head-only (backbone frozen)\n",
    "m_head_only = build_model({**config, 'freeze_policy': 'head_only'})\n",
    "print('head_only trainable:', count_params(m_head_only, trainable_only=True))\n",
    "\n",
    "# Full fine-tuning\n",
    "m_all = build_model({**config, 'freeze_policy': 'finetune_all'})\n",
    "print('finetune_all trainable:', count_params(m_all, trainable_only=True))\n",
    "\n",
    "# Last blocks only (optional)\n",
    "m_last = build_model({**config, 'freeze_policy': 'last_blocks_only', 'last_n_blocks': 2})\n",
    "print('last_blocks_only trainable:', count_params(m_last, trainable_only=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
