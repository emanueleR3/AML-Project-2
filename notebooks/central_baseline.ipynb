{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# AML Project 2: Protocol Step 2 - Central Baseline (Backbone Fine-tuning)\n",
                "\n",
                "This notebook trains the central baseline according to the new protocol.\n",
                "- **Pre-requisite:** `pretrained_head.pt` (Protocol Step 1)\n",
                "- **Backbone:** Unfrozen (`finetune_all`)\n",
                "- **Classifier:** Frozen (`freeze_head=True`)\n",
                "- **Epochs:** 40\n",
                "- **Output:** `output/main/central_baseline.pt`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Repository & Install Dependencies\n",
                "!git clone https://github.com/emanueleR3/AML-Project-2.git\n",
                "%cd AML-Project-2\n",
                "!pip install -r requirements.txt\n",
                "!pip install torch torchvision numpy matplotlib tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports & Setup\n",
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "from src.utils import set_seed, get_device, ensure_dir, save_checkpoint, save_metrics_json, count_parameters\n",
                "from src.data import load_cifar100, create_dataloader\n",
                "from src.model import build_model\n",
                "from src.train import evaluate, train_one_epoch\n",
                "\n",
                "sys.path.append('.')\n",
                "\n",
                "# Setup output dirs\n",
                "OUTPUT_DIR = 'output/main'\n",
                "ensure_dir(OUTPUT_DIR)\n",
                "device = get_device()\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# Set seed for reproducibility\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "print(\"Loading CIFAR-100...\")\n",
                "train_trainval, test_dataset = load_cifar100(data_dir='./data', image_size=224, download=True)\n",
                "\n",
                "# Split Train/Val\n",
                "train_size = int(0.9 * len(train_trainval))\n",
                "val_size = len(train_trainval) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(\n",
                "    train_trainval, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
                ")\n",
                "\n",
                "# Create loaders\n",
                "train_loader = create_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
                "val_loader = create_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
                "test_loader = create_dataloader(test_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Configuration\n",
                "config = {\n",
                "    'model_name': 'dino_vits16',\n",
                "    'num_classes': 100,\n",
                "    'freeze_policy': 'finetune_all',  # Unfreeze backbone\n",
                "    'freeze_head': True,              # Freeze classifier (Protocol Step 2)\n",
                "    'dropout': 0.1,\n",
                "    'device': device\n",
                "}\n",
                "\n",
                "model = build_model(config)\n",
                "model.to(device)\n",
                "\n",
                "# Load Pre-trained Head\n",
                "head_path = os.path.join(OUTPUT_DIR, 'pretrained_head.pt')\n",
                "if os.path.exists(head_path):\n",
                "    ckpt = torch.load(head_path, map_location=device)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    print(f\"✓ Loaded pre-trained head from {head_path}\")\n",
                "else:\n",
                "    raise FileNotFoundError(\"Please run pretrain_head.ipynb first!\")\n",
                "\n",
                "print(f\"Total parameters: {count_parameters(model):,}\")\n",
                "print(f\"Trainable parameters: {count_parameters(model, trainable_only=True):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sanity_check",
            "metadata": {},
            "outputs": [],
            "source": [
                "# DIAGNOSTIC: Verify pretrained model accuracy BEFORE training\n",
                "print(\"\\n=== SANITY CHECK: Pretrained Model Performance ===\")\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    val_loss, val_acc = evaluate(model, val_loader, criterion, device, show_progress=False)\n",
                "print(f\"Pretrained Model Val Accuracy: {val_acc:.2f}%\")\n",
                "print(\"(Should be ~75-85% if pretrained head and DINO backbone are working)\")\n",
                "print(\"(If ~1%, the pretrained_head.pt may be corrupted or mismatched)\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_baseline",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "epochs = 40\n",
                "eval_freq = 5\n",
                "\n",
                "# lr=1e-4 for fine-tuning Vision Transformers with SGD\n",
                "optimizer = torch.optim.SGD(model.get_trainable_params(), lr=1e-5, momentum=0.9, weight_decay=1e-4)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "best_acc = 0.0\n",
                "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "model.train()\n",
                "print(f\"Starting Central Backbone Training for {epochs} epochs...\")\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    loss, acc = train_one_epoch(model, train_loader, optimizer, criterion, device, show_progress=False)\n",
                "    \n",
                "    # Validation logic\n",
                "    current_epoch = epoch + 1\n",
                "    if current_epoch % eval_freq == 0 or current_epoch == epochs: \n",
                "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, show_progress=False)\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            save_checkpoint({'model_state_dict': model.state_dict()}, os.path.join(OUTPUT_DIR, 'central_baseline.pt'))\n",
                "            \n",
                "        print(f\"Epoch {current_epoch}/{epochs} | Train Acc: {acc:.2f}% | Val Acc: {val_acc:.2f}% | Best: {best_acc:.2f}%\")\n",
                "        \n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "\n",
                "    else:\n",
                "        print(f\"Epoch {current_epoch}/{epochs} | Train Acc: {acc:.2f}% | (Skipping Eval)\")\n",
                "    \n",
                "    scheduler.step()\n",
                "    \n",
                "    history['train_loss'].append(loss)\n",
                "    history['train_acc'].append(acc)\n",
                "\n",
                "print(f\"\\nBaseline finished. Best Val Acc: {best_acc:.2f}%\")\n",
                "\n",
                "save_metrics_json(os.path.join(OUTPUT_DIR, 'central_baseline_metrics.json'), history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_eval",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Test Evaluation\n",
                "print(\"\\nEvaluating on Test Set...\")\n",
                "# Load best model\n",
                "ckpt = torch.load(os.path.join(OUTPUT_DIR, 'central_baseline.pt'), map_location=device)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "\n",
                "test_loss, test_acc = evaluate(model, test_loader, criterion, device, show_progress=True)\n",
                "print(f\"\\n✓ Final Test Accuracy: {test_acc:.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
